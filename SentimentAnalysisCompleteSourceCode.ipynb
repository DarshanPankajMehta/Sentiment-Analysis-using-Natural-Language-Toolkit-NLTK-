{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SentimentAnalysisSelfExercise.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvMAtPqsz1dI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "fc92ad21-6160-4fc2-f27f-9c00253d7cfc"
      },
      "source": [
        "import nltk\n",
        "nltk.download('twitter_samples')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeN-P-ISz3tL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import twitter_samples\n",
        "'''\n",
        "This will import three datasets from NLTK that contain various tweets to train and test the model:\n",
        "\n",
        "negative_tweets.json: 5000 tweets with negative sentiments\n",
        "positive_tweets.json: 5000 tweets with positive sentiments\n",
        "tweets.20150430-223406.json: 20000 tweets with no sentiments\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8o_ZyOwD0yIQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
        "text = twitter_samples.strings('tweets.20150430-223406.json')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JghGgBMc1AQF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "5f86a09e-5da7-4c3a-a85b-0d530d2e89e2"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsWpkOBN2JHa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweet_tokens = twitter_samples.tokenized('positive_tweets.json')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOCVjS1N3QoK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "acaa0dd8-4efa-4444-cd13-1a8b1f12fda6"
      },
      "source": [
        "print(tweet_tokens[0])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['#FollowFriday', '@France_Inte', '@PKuchly57', '@Milipol_Paris', 'for', 'being', 'top', 'engaged', 'members', 'in', 'my', 'community', 'this', 'week', ':)']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRo6cSgA3Xcu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "9d7dc50e-d837-4f18-a63c-c9a88ab056c5"
      },
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hthLojgI5Vtq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tag import pos_tag"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGYCHEgq5rqw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "578490d5-2418-431c-ef25-6cb831fdff9f"
      },
      "source": [
        "print(pos_tag(tweet_tokens[0]))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('#FollowFriday', 'JJ'), ('@France_Inte', 'NNP'), ('@PKuchly57', 'NNP'), ('@Milipol_Paris', 'NNP'), ('for', 'IN'), ('being', 'VBG'), ('top', 'JJ'), ('engaged', 'VBN'), ('members', 'NNS'), ('in', 'IN'), ('my', 'PRP$'), ('community', 'NN'), ('this', 'DT'), ('week', 'NN'), (':)', 'NN')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDL_sNZr5w8Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "8cc78f2b-f5d9-491a-da4d-27360e00ed26"
      },
      "source": [
        "'''\n",
        "Alphabetical list of part-of-speech tags used in the Penn Treebank Project:\n",
        "Number  Tag Description\n",
        "1.\t    CC\tCoordinating conjunction\n",
        "2.\t    CD\tCardinal number\n",
        "3.\t    DT\tDeterminer\n",
        "4.\t    EX\tExistential there\n",
        "5.\t    FW\tForeign word\n",
        "6.\t    IN\tPreposition or subordinating conjunction\n",
        "7.\t    JJ\tAdjective\n",
        "8.\t    JJR\tAdjective, comparative\n",
        "9.\t    JJS\tAdjective, superlative\n",
        "10.\t    LS\tList item marker\n",
        "11.\t    MD\tModal\n",
        "12.\t    NN\tNoun, singular or mass\n",
        "13.\t    NNS\tNoun, plural\n",
        "14.\t    NNP\tProper noun, singular\n",
        "15.\t    NNPS\tProper noun, plural\n",
        "16.\t    PDT\tPredeterminer\n",
        "17.\t    POS\tPossessive ending\n",
        "18.\t    PRP\tPersonal pronoun\n",
        "19.\t    PRP$\tPossessive pronoun\n",
        "20.\t    RB\tAdverb\n",
        "21.\t    RBR\tAdverb, comparative\n",
        "22.\t    RBS\tAdverb, superlative\n",
        "23.\t    RP\tParticle\n",
        "24.\t    SYM\tSymbol\n",
        "25.\t    TO\tto\n",
        "26.\t    UH\tInterjection\n",
        "27.\t    VB\tVerb, base form\n",
        "28.\t    VBD\tVerb, past tense\n",
        "29.\t    VBG\tVerb, gerund or present participle\n",
        "30.\t    VBN\tVerb, past participle\n",
        "31.\t    VBP\tVerb, non-3rd person singular present\n",
        "32.\t    VBZ\tVerb, 3rd person singular present\n",
        "33.\t    WDT\tWh-determiner\n",
        "34.\t    WP\tWh-pronoun\n",
        "35.\t    WP$\tPossessive wh-pronoun\n",
        "36.\t    WRB\tWh-adverb\n",
        "'''"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nCC coordinating conjunction\\nCD cardinal digit\\nDT determiner\\nEX existential there (like: “there is” … think of it like “there exists”)\\nFW foreign word\\nIN preposition/subordinating conjunction\\nJJ adjective ‘big’\\nJJR adjective, comparative ‘bigger’\\nJJS adjective, superlative ‘biggest’\\nLS list marker 1)\\nMD modal could, will\\nNN noun, singular ‘desk’\\nNNS noun plural ‘desks’\\nNNP proper noun, singular ‘Harrison’\\nNNPS proper noun, plural ‘Americans’\\nPDT predeterminer ‘all the kids’\\nPOS possessive ending parent’s\\nPRP personal pronoun I, he, she\\nPRP$ possessive pronoun my, his, hers\\nRB adverb very, silently,\\nRBR adverb, comparative better\\nRBS adverb, superlative best\\nRP particle give up\\nTO, to go ‘to’ the store.\\nUH interjection, errrrrrrrm\\nVB verb, base form take\\nVBD verb, past tense, took\\nVBG verb, gerund/present participle taking\\nVBN verb, past participle is taken\\nVBP verb, sing. present, known-3d take\\nVBZ verb, 3rd person sing. present takes\\nWDT wh-determiner which\\nWP wh-pronoun who, what\\nWP$ possessive wh-pronoun whose\\nWRB wh-adverb where, when\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QagBIWi6ZBw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "969fb260-5fc1-43f6-9d2c-80e674ea7817"
      },
      "source": [
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "def lemmatize_sentence(tokens):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_sentence = []\n",
        "    for word, tag in pos_tag(tokens):\n",
        "        if tag.startswith('NN'):\n",
        "            pos = 'n'\n",
        "        elif tag.startswith('VB'):\n",
        "            pos = 'v'\n",
        "        else:\n",
        "            pos = 'a'\n",
        "        lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n",
        "    return lemmatized_sentence\n",
        "\n",
        "print(lemmatize_sentence(tweet_tokens[0]))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['#FollowFriday', '@France_Inte', '@PKuchly57', '@Milipol_Paris', 'for', 'be', 'top', 'engage', 'member', 'in', 'my', 'community', 'this', 'week', ':)']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNslzFhu7kkP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re, string\n",
        "\n",
        "def remove_noise(tweet_tokens, stop_words = ()):\n",
        "\n",
        "    cleaned_tokens = []\n",
        "\n",
        "    for token, tag in pos_tag(tweet_tokens):\n",
        "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
        "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
        "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
        "\n",
        "        if tag.startswith(\"NN\"):\n",
        "            pos = 'n'\n",
        "        elif tag.startswith('VB'):\n",
        "            pos = 'v'\n",
        "        else:\n",
        "            pos = 'a'\n",
        "\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        token = lemmatizer.lemmatize(token, pos)\n",
        "\n",
        "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
        "            cleaned_tokens.append(token.lower())\n",
        "    return cleaned_tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOflSzZN9xJD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "accd6c53-f5c7-4a61-98d4-44ef957b930a"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClUEQmUU9zOv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "549014a0-c725-4ca2-ec95-e81716db422e"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "print(remove_noise(tweet_tokens[0], stop_words))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['#followfriday', 'top', 'engage', 'member', 'community', 'week', ':)']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ILDgnKE93Ax",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
        "negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n",
        "\n",
        "positive_cleaned_tokens_list = []\n",
        "negative_cleaned_tokens_list = []\n",
        "\n",
        "for tokens in positive_tweet_tokens:\n",
        "    positive_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
        "\n",
        "for tokens in negative_tweet_tokens:\n",
        "    negative_cleaned_tokens_list.append(remove_noise(tokens, stop_words))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-dAOybv-Hoa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "09ac83ae-8424-4ea3-e91a-4f2d350ae537"
      },
      "source": [
        "print(positive_tweet_tokens[100])\n",
        "print(positive_cleaned_tokens_list[100])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['@metalgear_jp', '@Kojima_Hideo', 'I', 'want', \"you're\", 'T-shirts', '!', 'They', 'are', 'so', 'cool', '!', ':D']\n",
            "['want', 't-shirts', 'cool', ':d']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6ELVArD-q4o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_all_words(cleaned_tokens_list):\n",
        "    for tokens in cleaned_tokens_list:\n",
        "        for token in tokens:\n",
        "            yield token\n",
        "\n",
        "all_pos_words = get_all_words(positive_cleaned_tokens_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYOiEq9e_SId",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b4eb8020-b422-400a-b909-e6759d7669a8"
      },
      "source": [
        "from nltk import FreqDist\n",
        "\n",
        "freq_dist_pos = FreqDist(all_pos_words)\n",
        "print(freq_dist_pos.most_common(10))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(':)', 3691), (':-)', 701), (':d', 658), ('thanks', 388), ('follow', 357), ('love', 333), ('...', 290), ('good', 283), ('get', 263), ('thank', 253)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfTgLRmM_Zmc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_tweets_for_model(cleaned_tokens_list):\n",
        "    for tweet_tokens in cleaned_tokens_list:\n",
        "        yield dict([token, True] for token in tweet_tokens)\n",
        "\n",
        "positive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\n",
        "negative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xF6GF-ROAGes",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "\n",
        "positive_dataset = [(tweet_dict, \"Positive\")\n",
        "                     for tweet_dict in positive_tokens_for_model]\n",
        "\n",
        "negative_dataset = [(tweet_dict, \"Negative\")\n",
        "                     for tweet_dict in negative_tokens_for_model]\n",
        "\n",
        "dataset = positive_dataset + negative_dataset\n",
        "\n",
        "random.shuffle(dataset)\n",
        "\n",
        "train_data = dataset[:7000]\n",
        "test_data = dataset[7000:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmK8PPzlAQNT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "f70a2114-88e5-4b98-cd81-781240aa3663"
      },
      "source": [
        "from nltk import classify\n",
        "from nltk import NaiveBayesClassifier\n",
        "classifier = NaiveBayesClassifier.train(train_data)\n",
        "\n",
        "print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n",
        "\n",
        "print(classifier.show_most_informative_features(10))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy is: 0.9966666666666667\n",
            "Most Informative Features\n",
            "                      :) = True           Positi : Negati =    988.3 : 1.0\n",
            "                follower = True           Positi : Negati =     24.0 : 1.0\n",
            "                 welcome = True           Positi : Negati =     21.2 : 1.0\n",
            "                     sad = True           Negati : Positi =     20.2 : 1.0\n",
            "               community = True           Positi : Negati =     16.5 : 1.0\n",
            "                   enjoy = True           Positi : Negati =     15.7 : 1.0\n",
            "                     bam = True           Positi : Negati =     15.1 : 1.0\n",
            "                     x15 = True           Negati : Positi =     14.9 : 1.0\n",
            "                followed = True           Negati : Positi =     14.9 : 1.0\n",
            "              appreciate = True           Positi : Negati =     13.1 : 1.0\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHP7QJ0tAhpW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4ff5a92c-534a-40ee-8adf-2ec289be2f76"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "custom_tweet = \"I ordered just once from TerribleCo, they screwed up, never used the app again.\"\n",
        "\n",
        "custom_tokens = remove_noise(word_tokenize(custom_tweet))\n",
        "\n",
        "print(classifier.classify(dict([token, True] for token in custom_tokens)))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Negative\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17rWIerABBAp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9b38e139-29ba-4b12-fadb-e80ee92651fd"
      },
      "source": [
        "custom_tweet = 'Congrats #SportStar on your 7th best goal from last season winning goal of the year :) #Baller #Topbin #oneofmanyworldies'\n",
        "custom_tokens = remove_noise(word_tokenize(custom_tweet))\n",
        "\n",
        "print(classifier.classify(dict([token, True] for token in custom_tokens)))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXtn_GGbBK5s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3ea0f24a-686f-4e96-e41b-78a1d385f0cb"
      },
      "source": [
        "custom_tweet = 'Thank you for sending my baggage to CityX and flying me to CityY at the same time. Brilliant service. #thanksGenericAirline'\n",
        "custom_tokens = remove_noise(word_tokenize(custom_tweet))\n",
        "\n",
        "print(classifier.classify(dict([token, True] for token in custom_tokens)))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5g81Rh1nBUXm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Cleaned code:\n",
        "'''from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.corpus import twitter_samples, stopwords\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import FreqDist, classify, NaiveBayesClassifier\n",
        "\n",
        "import re, string, random\n",
        "\n",
        "def remove_noise(tweet_tokens, stop_words = ()):\n",
        "\n",
        "    cleaned_tokens = []\n",
        "\n",
        "    for token, tag in pos_tag(tweet_tokens):\n",
        "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
        "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
        "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
        "\n",
        "        if tag.startswith(\"NN\"):\n",
        "            pos = 'n'\n",
        "        elif tag.startswith('VB'):\n",
        "            pos = 'v'\n",
        "        else:\n",
        "            pos = 'a'\n",
        "\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        token = lemmatizer.lemmatize(token, pos)\n",
        "\n",
        "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
        "            cleaned_tokens.append(token.lower())\n",
        "    return cleaned_tokens\n",
        "\n",
        "def get_all_words(cleaned_tokens_list):\n",
        "    for tokens in cleaned_tokens_list:\n",
        "        for token in tokens:\n",
        "            yield token\n",
        "\n",
        "def get_tweets_for_model(cleaned_tokens_list):\n",
        "    for tweet_tokens in cleaned_tokens_list:\n",
        "        yield dict([token, True] for token in tweet_tokens)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "    negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
        "    text = twitter_samples.strings('tweets.20150430-223406.json')\n",
        "    tweet_tokens = twitter_samples.tokenized('positive_tweets.json')[0]\n",
        "\n",
        "    stop_words = stopwords.words('english')\n",
        "\n",
        "    positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
        "    negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n",
        "\n",
        "    positive_cleaned_tokens_list = []\n",
        "    negative_cleaned_tokens_list = []\n",
        "\n",
        "    for tokens in positive_tweet_tokens:\n",
        "        positive_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
        "\n",
        "    for tokens in negative_tweet_tokens:\n",
        "        negative_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
        "\n",
        "    all_pos_words = get_all_words(positive_cleaned_tokens_list)\n",
        "\n",
        "    freq_dist_pos = FreqDist(all_pos_words)\n",
        "    print(freq_dist_pos.most_common(10))\n",
        "\n",
        "    positive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\n",
        "    negative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)\n",
        "\n",
        "    positive_dataset = [(tweet_dict, \"Positive\")\n",
        "                         for tweet_dict in positive_tokens_for_model]\n",
        "\n",
        "    negative_dataset = [(tweet_dict, \"Negative\")\n",
        "                         for tweet_dict in negative_tokens_for_model]\n",
        "\n",
        "    dataset = positive_dataset + negative_dataset\n",
        "\n",
        "    random.shuffle(dataset)\n",
        "\n",
        "    train_data = dataset[:7000]\n",
        "    test_data = dataset[7000:]\n",
        "\n",
        "    classifier = NaiveBayesClassifier.train(train_data)\n",
        "\n",
        "    print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n",
        "\n",
        "    print(classifier.show_most_informative_features(10))\n",
        "\n",
        "    custom_tweet = \"I ordered just once from TerribleCo, they screwed up, never used the app again.\"\n",
        "\n",
        "    custom_tokens = remove_noise(word_tokenize(custom_tweet))\n",
        "\n",
        "    print(custom_tweet, classifier.classify(dict([token, True] for token in custom_tokens)))\n",
        "    '''"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}